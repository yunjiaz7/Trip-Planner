"""
LLMæœåŠ¡æµ‹è¯•è„šæœ¬

ç›®çš„ï¼š
1. éªŒè¯LLMæœåŠ¡æ˜¯å¦èƒ½æ­£å¸¸åˆå§‹åŒ–
2. æµ‹è¯•LLMè¿æ¥å’Œè°ƒç”¨åŠŸèƒ½
3. éªŒè¯LangChain ChatOpenAIæ˜¯å¦æ­£å¸¸å·¥ä½œ

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_llm_service.py
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.services.llm_service import get_llm, reset_llm
from app.config import get_settings


def test_llm_initialization():
    """æµ‹è¯•LLMæœåŠ¡åˆå§‹åŒ–"""
    print("=" * 60)
    print("æµ‹è¯•1: LLMæœåŠ¡åˆå§‹åŒ–")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("LLM_BASE_URL")
    model = os.getenv("OPENAI_MODEL") or os.getenv("LLM_MODEL_ID")
    
    print(f"ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    print(f"  API Key: {'å·²é…ç½®' if api_key else 'âŒ æœªé…ç½®'}")
    print(f"  Base URL: {base_url or 'ä½¿ç”¨é»˜è®¤'}")
    print(f"  Model: {model or 'ä½¿ç”¨é»˜è®¤'}")
    
    if not api_key:
        print("\nâš ï¸  è­¦å‘Š: API Keyæœªé…ç½®ï¼ŒLLMåˆå§‹åŒ–å¯èƒ½ä¼šå¤±è´¥")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: OPENAI_API_KEY æˆ– LLM_API_KEY")
        return False
    
    try:
        # é‡ç½®LLMå®ä¾‹ï¼ˆç¡®ä¿é‡æ–°åˆå§‹åŒ–ï¼‰
        reset_llm()
        
        # è·å–LLMå®ä¾‹
        llm = get_llm()
        
        print(f"\nâœ… LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        print(f"   LLMç±»å‹: {type(llm).__name__}")
        print(f"   æ¨¡å‹åç§°: {llm.model_name if hasattr(llm, 'model_name') else 'N/A'}")
        print(f"   æ¸©åº¦: {llm.temperature if hasattr(llm, 'temperature') else 'N/A'}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯LangChainçš„ChatOpenAI
        from langchain_openai import ChatOpenAI
        if isinstance(llm, ChatOpenAI):
            print(f"   âœ… æ˜¯LangChain ChatOpenAIå®ä¾‹")
        else:
            print(f"   âš ï¸  ä¸æ˜¯ChatOpenAIå®ä¾‹ï¼Œç±»å‹: {type(llm)}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_simple_call():
    """æµ‹è¯•LLMç®€å•è°ƒç”¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: LLMç®€å•è°ƒç”¨")
    print("=" * 60)
    
    try:
        llm = get_llm()
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        print("å‘é€æµ‹è¯•æ¶ˆæ¯: 'Hello, please respond with just OK'")
        
        from langchain_core.messages import HumanMessage
        
        response = llm.invoke([HumanMessage(content="Hello, please respond with just OK")])
        
        print(f"\nâœ… LLMè°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response).__name__}")
        print(f"   å“åº”å†…å®¹: {response.content if hasattr(response, 'content') else str(response)}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_streaming():
    """æµ‹è¯•LLMæµå¼è°ƒç”¨ï¼ˆå¯é€‰ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: LLMæµå¼è°ƒç”¨ï¼ˆå¯é€‰ï¼‰")
    print("=" * 60)
    
    try:
        llm = get_llm()
        
        from langchain_core.messages import HumanMessage
        
        print("å‘é€æµå¼è¯·æ±‚: 'Count from 1 to 5'")
        print("æµå¼å“åº”:")
        
        chunks = []
        for chunk in llm.stream([HumanMessage(content="Count from 1 to 5")]):
            if hasattr(chunk, 'content'):
                content = chunk.content
                print(content, end='', flush=True)
                chunks.append(content)
            else:
                print(str(chunk), end='', flush=True)
                chunks.append(str(chunk))
        
        print(f"\n\nâœ… æµå¼è°ƒç”¨æˆåŠŸ")
        print(f"   æ€»å“åº”é•¿åº¦: {len(''.join(chunks))} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
        print("   æ³¨æ„: æµå¼è°ƒç”¨å¤±è´¥ä¸å½±å“åŸºæœ¬åŠŸèƒ½")
        import traceback
        traceback.print_exc()
        return False


def test_llm_with_tools():
    """æµ‹è¯•LLMå·¥å…·è°ƒç”¨èƒ½åŠ›ï¼ˆéªŒè¯Tool Callingï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: LLMå·¥å…·è°ƒç”¨èƒ½åŠ›")
    print("=" * 60)
    
    try:
        llm = get_llm()
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å·¥å…·
        from langchain.tools import BaseTool
        from pydantic import BaseModel, Field
        from typing import Type  # å¯¼å…¥Typeç”¨äºç±»å‹æ³¨è§£
        
        class CalculatorInput(BaseModel):
            a: int = Field(description="First number")
            b: int = Field(description="Second number")
        
        class CalculatorTool(BaseTool):
            name: str = "calculator"  # æ·»åŠ ç±»å‹æ³¨è§£ï¼ˆPydantic 2.xè¦æ±‚ï¼‰
            description: str = "Adds two numbers together"  # æ·»åŠ ç±»å‹æ³¨è§£
            args_schema: Type[BaseModel] = CalculatorInput  # æ·»åŠ ç±»å‹æ³¨è§£
            
            def _run(self, a: int, b: int) -> str:
                return str(a + b)
        
        tool = CalculatorTool()
        
        # ç»‘å®šå·¥å…·åˆ°LLM
        llm_with_tools = llm.bind_tools([tool])
        
        print("å‘é€å¸¦å·¥å…·çš„è¯·æ±‚: 'What is 2 + 3?'")
        
        from langchain_core.messages import HumanMessage
        
        response = llm_with_tools.invoke([HumanMessage(content="What is 2 + 3?")])
        
        print(f"\nâœ… å·¥å…·è°ƒç”¨æµ‹è¯•æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response).__name__}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"   âœ… æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(response.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(response.tool_calls):
                print(f"      å·¥å…·è°ƒç”¨ {i+1}: {tool_call.get('name', 'unknown')}")
        else:
            print(f"   âš ï¸  æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼ˆå¯èƒ½LLMç›´æ¥å›ç­”äº†ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å·¥å…·è°ƒç”¨æµ‹è¯•å¤±è´¥: {str(e)}")
        print("   æ³¨æ„: å·¥å…·è°ƒç”¨å¤±è´¥å¯èƒ½ä¸å½±å“åŸºæœ¬åŠŸèƒ½")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("LLMæœåŠ¡æµ‹è¯• (LangChainç‰ˆæœ¬)")
    print("=" * 60)
    
    results = []
    
    # æµ‹è¯•1: LLMåˆå§‹åŒ–
    results.append(("LLMæœåŠ¡åˆå§‹åŒ–", test_llm_initialization()))
    
    # å¦‚æœåˆå§‹åŒ–æˆåŠŸï¼Œç»§ç»­å…¶ä»–æµ‹è¯•
    if results[0][1]:
        # æµ‹è¯•2: ç®€å•è°ƒç”¨
        results.append(("LLMç®€å•è°ƒç”¨", test_llm_simple_call()))
        
        # æµ‹è¯•3: æµå¼è°ƒç”¨ï¼ˆå¯é€‰ï¼‰
        try:
            results.append(("LLMæµå¼è°ƒç”¨", test_llm_streaming()))
        except Exception as e:
            print(f"\nâš ï¸  æµå¼è°ƒç”¨æµ‹è¯•è·³è¿‡: {str(e)}")
            results.append(("LLMæµå¼è°ƒç”¨", False))
        
        # æµ‹è¯•4: å·¥å…·è°ƒç”¨èƒ½åŠ›
        try:
            results.append(("LLMå·¥å…·è°ƒç”¨èƒ½åŠ›", test_llm_with_tools()))
        except Exception as e:
            print(f"\nâš ï¸  å·¥å…·è°ƒç”¨æµ‹è¯•è·³è¿‡: {str(e)}")
            results.append(("LLMå·¥å…·è°ƒç”¨èƒ½åŠ›", False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMæœåŠ¡è¿ç§»æˆåŠŸ")
        return 0
    elif passed > 0:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼ŒåŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return 0
    else:
        print("\nâŒ æ‰€æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1


if __name__ == "__main__":
    exit(main())
